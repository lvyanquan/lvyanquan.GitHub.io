{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://lvyanquan.github.io","root":"/"},"pages":[{"title":"分类","date":"2023-03-12T07:08:34.000Z","updated":"2023-03-12T07:41:41.758Z","comments":false,"path":"categories/index.html","permalink":"http://lvyanquan.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-05-22T02:44:35.746Z","updated":"2023-05-22T02:44:35.746Z","comments":true,"path":"links/index.html","permalink":"http://lvyanquan.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-03-12T07:11:01.000Z","updated":"2023-03-12T07:41:41.758Z","comments":false,"path":"tags/index.html","permalink":"http://lvyanquan.github.io/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2023-03-12T07:05:01.000Z","updated":"2023-04-26T09:01:04.996Z","comments":false,"path":"about/index.html","permalink":"http://lvyanquan.github.io/about/index.html","excerpt":"","text":"夫祸患常积于忽微，而智勇多困于所溺"}],"posts":[{"title":"Hudi cdc源码分析","slug":"Hudi cdc源码分析","date":"2023-05-19T09:38:10.560Z","updated":"2023-05-19T09:54:59.325Z","comments":true,"path":"2023/05/19/Hudi cdc源码分析/","link":"","permalink":"http://lvyanquan.github.io/2023/05/19/Hudi%20cdc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"原理部分参考这篇博客 HoodieMergeHandleFactory在开启cdc时创建HoodieMergeHandleWithChangeLog,需要设置参数”hoodie.table.cdc.enabled”为true。 12345678910111213141516171819202122232425262728public static &lt;T, I, K, O&gt; HoodieMergeHandle&lt;T, I, K, O&gt; create( WriteOperationType operationType, HoodieWriteConfig writeConfig, String instantTime, HoodieTable&lt;T, I, K, O&gt; table, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr, String partitionPath, String fileId, TaskContextSupplier taskContextSupplier, Option&lt;BaseKeyGenerator&gt; keyGeneratorOpt) &#123; if (table.requireSortedRecords()) &#123; if (table.getMetaClient().getTableConfig().isCDCEnabled()) &#123; return new HoodieSortedMergeHandleWithChangeLog&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt); &#125; else &#123; return new HoodieSortedMergeHandle&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt); &#125; &#125; else if (!WriteOperationType.isChangingRecords(operationType) &amp;&amp; writeConfig.allowDuplicateInserts()) &#123; return new HoodieConcatHandle&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt); &#125; else &#123; if (table.getMetaClient().getTableConfig().isCDCEnabled()) &#123; return new HoodieMergeHandleWithChangeLog&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt); &#125; else &#123; return new HoodieMergeHandle&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt); &#125; &#125;&#125; HoodieMergeHandleWithChangeLog 初始化时会创建一个HoodieCDCLogger对象。在update和insert数据时写入变更数据，在insert数据的情况下，oldRecord写入null。 1234567891011121314151617181920212223protected boolean writeUpdateRecord(HoodieRecord&lt;T&gt; newRecord, HoodieRecord&lt;T&gt; oldRecord, Option&lt;HoodieRecord&gt; combinedRecordOpt, Schema writerSchema) throws IOException &#123; // TODO [HUDI-5019] Remove these unnecessary newInstance invocations Option&lt;HoodieRecord&gt; savedCombineRecordOp = combinedRecordOpt.map(HoodieRecord::newInstance); final boolean result = super.writeUpdateRecord(newRecord, oldRecord, combinedRecordOpt, writerSchema); if (result) &#123; boolean isDelete = HoodieOperation.isDelete(newRecord.getOperation()); Option&lt;IndexedRecord&gt; avroRecordOpt = savedCombineRecordOp.flatMap(r -&gt; toAvroRecord(r, writerSchema, config.getPayloadConfig().getProps())); cdcLogger.put(newRecord, (GenericRecord) oldRecord.getData(), isDelete ? Option.empty() : avroRecordOpt); &#125; return result;&#125;protected void writeInsertRecord(HoodieRecord&lt;T&gt; newRecord) throws IOException &#123; Schema schema = useWriterSchemaForCompaction ? writeSchemaWithMetaFields : writeSchema; // TODO Remove these unnecessary newInstance invocations HoodieRecord&lt;T&gt; savedRecord = newRecord.newInstance(); super.writeInsertRecord(newRecord); if (!HoodieOperation.isDelete(newRecord.getOperation())) &#123; cdcLogger.put(newRecord, null, savedRecord.toIndexedRecord(schema, config.getPayloadConfig().getProps()).map(HoodieAvroIndexedRecord::getData)); &#125;&#125; HoodieCDCLogger 通过比较oldRecord和newRecord判断这条数据的操作类型，这里写入的是recordKey和完整的record数据。 123456789101112131415161718192021222324252627282930public void put(HoodieRecord hoodieRecord, GenericRecord oldRecord, Option&lt;IndexedRecord&gt; newRecord) &#123; String recordKey = hoodieRecord.getRecordKey(); GenericData.Record cdcRecord; if (newRecord.isPresent()) &#123; GenericRecord record = (GenericRecord) newRecord.get(); if (oldRecord == null) &#123; // INSERT cdc record cdcRecord = this.transformer.transform(HoodieCDCOperation.INSERT, recordKey, null, record); &#125; else &#123; // UPDATE cdc record cdcRecord = this.transformer.transform(HoodieCDCOperation.UPDATE, recordKey, oldRecord, record); &#125; &#125; else &#123; // DELETE cdc record cdcRecord = this.transformer.transform(HoodieCDCOperation.DELETE, recordKey, oldRecord, null); &#125; flushIfNeeded(false); HoodieAvroPayload payload = new HoodieAvroPayload(Option.of(cdcRecord)); if (cdcData.isEmpty()) &#123; averageCDCRecordSize = sizeEstimator.sizeEstimate(payload); &#125; cdcData.put(recordKey, payload); numOfCDCRecordsInMemory.incrementAndGet();&#125;","categories":[],"tags":[{"name":"Hudi","slug":"Hudi","permalink":"http://lvyanquan.github.io/tags/Hudi/"}]},{"title":"Hudi merge into多条件更新","slug":"Hudi merge into多条件更新","date":"2023-05-17T05:50:09.575Z","updated":"2023-05-17T05:50:09.626Z","comments":true,"path":"2023/05/17/Hudi merge into多条件更新/","link":"","permalink":"http://lvyanquan.github.io/2023/05/17/Hudi%20merge%20into%E5%A4%9A%E6%9D%A1%E4%BB%B6%E6%9B%B4%E6%96%B0/","excerpt":"","text":"验证这个jira 创建表 1234567891011121314151617create table base ( id int, price int) using huditblproperties ( primaryKey = &#x27;id&#x27;);create table change ( id int, price int) using huditblproperties ( primaryKey = &#x27;id&#x27;); 插入数据 1234insert into base values (1, 1), (2, 2);insert into change values (1, 10), (2, 20), (3, 30); 执行按条件更新(设置多种条件) 123456789101112merge into base as targetusing (select id, price from change) sourceon target.id = source.id/* 条件一 id=1时相加 */when matched and target.id = 1 thenupdate set target.price = source.price + target.price/* 条件二 设置为较大的price */when matched and target.price &lt; source.price thenupdate set target.price = source.price/* 不匹配时插入 */when not matched theninsert *; 查询base表数据 12320230517133958447 20230517133958447_0_0 2 c4be3a67-b9f4-4593-883f-9cca0f1e1e15-0_0-4-4_20230517133958447.parquet 2 2020230517133958447 20230517133958447_0_1 1 c4be3a67-b9f4-4593-883f-9cca0f1e1e15-0_0-4-4_20230517133958447.parquet 1 1120230517133958447 20230517133958447_0_2 3 c4be3a67-b9f4-4593-883f-9cca0f1e1e15-0_0-4-4_20230517133958447.parquet 3 30","categories":[],"tags":[{"name":"Hudi","slug":"Hudi","permalink":"http://lvyanquan.github.io/tags/Hudi/"}]},{"title":"使用anki构建个人单词学习库","slug":"使用anki构建个人单词学习库","date":"2023-05-12T08:37:54.842Z","updated":"2023-05-12T10:05:43.738Z","comments":true,"path":"2023/05/12/使用anki构建个人单词学习库/","link":"","permalink":"http://lvyanquan.github.io/2023/05/12/%E4%BD%BF%E7%94%A8anki%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%95%E8%AF%8D%E5%AD%A6%E4%B9%A0%E5%BA%93/","excerpt":"","text":"过去学习英语都是面向考试学习的，四六级、考研、雅思等，都有具体的单词书。但是在工作中，需要看的英语文档都是专业方向的，与通用的英语单词本重合度比较低。于是我找到了anki这么一个可以自定义单词内容的学习软件，这里把使用流程记录一下。 安装anki通过Anki官网，点击Download按钮跳转至页面底部。安装流程还是很简单的，安装好后的界面如下： 当然，这里的“英语单词”牌照是我自己添加的。简单介绍一下这个软件里的术语：卡片：卡片就是你设计的一个知识点，可以是单词，也可以是一首诗，自由定义。牌组：牌组就是卡片的集合，也就是知识库了。卡片模板：对于一个知识点，你想要以什么样的方式呈现，这可以通过设计模板实现。以基础的记单词模型为例，我们会给出单词的英文，思考记忆单词的含义，这就是问答题的模板，我们可以在“添加”这一栏里找到这个配置模板：并且根据我们的知识点填上正面和反面需要展示的内容：这样，一张卡片就完成了，当然，你可以需要预先创建一个知识库牌组。 安装浏览器插件但是，每次都自己输入单词的英文和中文释义也还是有些麻烦，鉴于我主要是在浏览器上查看英文文档，找了相关的浏览器插件支持一下。这里推荐的是在线词典助手,点击安装就可以了。安装好这个拓展以后，我们就要把这个浏览器插件跟anki绑定起来了，这里通过ankiconnect绑定，ankiconnect是一个插件，可以直接通过anki安装。在“工具”-“插件”-“获取插件”这里可以找到下载插件的地方，输入插件编号“2055492159”自动下载ankiconnect：然后去在线词典助手上配置绑定信息，以问答题为例，配置输出选项，选择单词词组在正面，单一释义在背面。当然，为了更好的应用辞典信息，只有英文单词和中文含义是不够的，所以最好还是要修改一下模板。这里选用这个模板 开始学习配置好浏览器插件后，对自己想要学习的单词点击鼠标右键，点选“➕”号把单词添加到牌组里。点击牌组，开始学习这个单词。正面：背面：","categories":[],"tags":[]},{"title":"LinkageError解决","slug":"LinkageError解决","date":"2023-05-10T11:35:42.700Z","updated":"2023-05-12T06:37:42.889Z","comments":true,"path":"2023/05/10/LinkageError解决/","link":"","permalink":"http://lvyanquan.github.io/2023/05/10/LinkageError%E8%A7%A3%E5%86%B3/","excerpt":"","text":"思路参考这个回答在解决依赖冲突的场景中，偶尔会遇到这个问题： 1234java.lang.LinkageError: loader constraint violation: when resolving method &quot;org.slf4j.impl.StaticLoggerBinder.getLoggerFactory()Lorg/slf4j/ILoggerFactory;&quot; the class loader (instance of org/apache/catalina/loader/WebappClassLoader) of the current class, org/slf4j/LoggerFactory, and the class loader (instance of org/apache/catalina/loader/StandardClassLoader) for resolved class, org/slf4j/impl/StaticLoggerBinder, have different Class objects for the type taticLoggerBinder.getLoggerFactory()Lorg/slf4j/ILoggerFactory; used in the signatureat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:299)at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:269)at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281) 显然，这是log4j的依赖冲突，但是通过idea的Mavendependency Analyzer并没有找到冲突的依赖，想办法排依赖可能就解决了。然而这种解决方式并不靠谱。这里把问题出现的原因和解决流程记录一下。 排查流程这个问题是ILoggerFactory这个类被加载了两次 在jvm参数里添加 -verbose:class， 打印类的加载路径找到冲突的jar包123[Loaded org.slf4j.ILoggerFactory from file:/Users/kunni/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar]......[Loaded org.slf4j.ILoggerFactory from file:/Users/kunni/.m2/repository/org/apache/hive/hive-exec/2.3.8/hive-exec-2.3.8.jar] 可以看到这个类在slf4j-api-1.7.30.jar和hive-exec-2.3.8.jar上都出现了，可以断定hive-exec-2.3.8.jar里包含了其他版本的slf4j依赖。 通过反编译查看依赖版本通过jd-gui查看hive-exec-2.3.8.jar里包含的slf4j依赖版本发现两个版本确实是不一致的，想办法解决这个问题。 解决 通过maven的依赖覆盖解决这个问题 通过maven shade插件改名解决这个问题1234567891011121314151617181920212223242526&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.4&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactSet&gt; &lt;includes&gt; &lt;include&gt;*:*&lt;/include&gt; &lt;/includes&gt; &lt;/artifactSet&gt; &lt;relocations&gt; &lt;relocation&gt; &lt;pattern&gt;org.slf4j..&lt;/pattern&gt; &lt;shadedPattern&gt;shade.org.slf4j.&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;/relocations&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;","categories":[],"tags":[{"name":"bugfix","slug":"bugfix","permalink":"http://lvyanquan.github.io/tags/bugfix/"},{"name":"Java","slug":"Java","permalink":"http://lvyanquan.github.io/tags/Java/"}]},{"title":"volatile关键字","slug":"volatile关键字","date":"2023-05-09T11:30:50.219Z","updated":"2023-05-10T02:01:58.686Z","comments":true,"path":"2023/05/09/volatile关键字/","link":"","permalink":"http://lvyanquan.github.io/2023/05/09/volatile%E5%85%B3%E9%94%AE%E5%AD%97/","excerpt":"","text":"来源有序性指的是程序按照代码的先后顺序执行。而编译器为了优化性能，有时候会改变程序中语句的先后顺序。Java 中经典的案例就是利用双重检查创建单例对象，其中 volatile 就是保证有序性的。 123456789101112131415public class Singleton &#123; private static volatile Singleton singleton; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 如果没有 volatile ，我们以为的 new 操作应该是： 分配一块内存 M ； 在内存 M 上初始化 Singleton 对象； 然后 M 的地址赋值给 instance 变量。 但是实际上优化后的执行路径却是这样的： 分配一块内存 M ； 将 M 的地址赋值给 instance 变量； 最后在内存 M 上初始化 Singleton 对象。 假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance !&#x3D; null ，所以直接返回 instance ，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://lvyanquan.github.io/tags/Java/"},{"name":"juc","slug":"juc","permalink":"http://lvyanquan.github.io/tags/juc/"}]},{"title":"FlinkCDC全增量读取源码分析","slug":"FlinkCDC全增量读取源码分析","date":"2023-05-06T11:36:30.312Z","updated":"2023-05-10T02:43:02.434Z","comments":true,"path":"2023/05/06/FlinkCDC全增量读取源码分析/","link":"","permalink":"http://lvyanquan.github.io/2023/05/06/FlinkCDC%E5%85%A8%E5%A2%9E%E9%87%8F%E8%AF%BB%E5%8F%96%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"[未完待续]快照读取的逻辑: SHOW MASTER STATUS 获取 lw，插入队列读取该分片内的记录，插入队列SHOW MASTER STATUS 获取 hw，插入队列判断 lw 与 hw 之间是否有增量变更如果没有变更，队列中插入 BINLOG_END 记录否则读取 [lw, hw] 之间的 binlog 并插入队列，最后一条记录为 BINLOG_ENDMySqlSnapshotSplitReadTask#doExecute 1234567891011121314151617181920212223242526272829303132protected SnapshotResult doExecute( ChangeEventSourceContext context, OffsetContext previousOffset, SnapshotContext snapshotContext, SnapshottingTask snapshottingTask) throws Exception &#123; final RelationalSnapshotChangeEventSource.RelationalSnapshotContext ctx = (RelationalSnapshotChangeEventSource.RelationalSnapshotContext) snapshotContext; ctx.offset = offsetContext; final BinlogOffset lowWatermark = currentBinlogOffset(jdbcConnection); LOG.info( &quot;Snapshot step 1 - Determining low watermark &#123;&#125; for split &#123;&#125;&quot;, lowWatermark, snapshotSplit); ((SnapshotSplitChangeEventSourceContext) (context)).setLowWatermark(lowWatermark); dispatcher.dispatchWatermarkEvent( offsetContext.getPartition(), snapshotSplit, lowWatermark, WatermarkKind.LOW); LOG.info(&quot;Snapshot step 2 - Snapshotting data&quot;); createDataEvents(ctx, snapshotSplit.getTableId()); final BinlogOffset highWatermark = currentBinlogOffset(jdbcConnection); LOG.info( &quot;Snapshot step 3 - Determining high watermark &#123;&#125; for split &#123;&#125;&quot;, highWatermark, snapshotSplit); ((SnapshotSplitChangeEventSourceContext) (context)).setHighWatermark(lowWatermark); dispatcher.dispatchWatermarkEvent( offsetContext.getPartition(), snapshotSplit, highWatermark, WatermarkKind.HIGH); return SnapshotResult.completed(ctx.offset);&#125;","categories":[],"tags":[{"name":"FlinkCDC","slug":"FlinkCDC","permalink":"http://lvyanquan.github.io/tags/FlinkCDC/"}]},{"title":"2418. 按身高排序","slug":"2418. 按身高排序","date":"2023-04-25T15:02:28.597Z","updated":"2023-04-26T09:16:37.422Z","comments":true,"path":"2023/04/25/2418. 按身高排序/","link":"","permalink":"http://lvyanquan.github.io/2023/04/25/2418.%20%E6%8C%89%E8%BA%AB%E9%AB%98%E6%8E%92%E5%BA%8F/","excerpt":"","text":"2418. 按身高排序 题目给你一个字符串数组 names ，和一个由 互不相同 的正整数组成的数组 heights 。两个数组的长度均为 n 。 对于每个下标 i，names[i] 和 heights[i] 表示第 i 个人的名字和身高。 请按身高 降序 顺序返回对应的名字数组 names 。 代码简单题稍微需要设计的是怎么将 name 和 height 绑定在一起。这里使用的是 Class，也可以使用 map 12345678910111213141516171819202122232425262728import java.util.*;class Solution &#123; public String[] sortPeople(String[] names, int[] heights) &#123; int n = heights.length; Pair[] pairs = new Pair[n]; for(int i=0;i&lt;n;i++) &#123; pairs[i] = new Pair(names[i], heights[i]); &#125; Arrays.sort(pairs, (Pair a, Pair b) -&gt; &#123; return Integer.compare(b.height, a.height); &#125;); String[] res = new String[n]; for(int i=0;i&lt;n;i++) &#123; res[i] = pairs[i].name; &#125; return res; &#125;&#125; class Pair &#123; String name; int height; Pair(String name, int height) &#123; this.name = name; this.height = height; &#125; &#125;","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"http://lvyanquan.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"linux 磁盘空间查看","slug":"linux 磁盘空间查看","date":"2023-03-15T02:53:31.010Z","updated":"2023-04-26T09:17:30.497Z","comments":true,"path":"2023/03/15/linux 磁盘空间查看/","link":"","permalink":"http://lvyanquan.github.io/2023/03/15/linux%20%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E6%9F%A5%E7%9C%8B/","excerpt":"","text":"查看某个分区下文件的存储情况 123456789du --max-dep=1 -h /示例：du --max-dep=1 -h /data/hadoop_admin/nm-local-dir/38G /data/hadoop_admin/nm-local-dir/usercache56K /data/hadoop_admin/nm-local-dir/nmPrivate5.6M /data/hadoop_admin/nm-local-dir/filecache38G /data/hadoop_admin/nm-local-dir/ 查看磁盘使用情况 1234567891011 df -h文件系统 容量 已用 可用 已用% 挂载点/dev/vda3 116G 39G 77G 34% /devtmpfs 16G 0 16G 0% /devtmpfs 16G 0 16G 0% /dev/shmtmpfs 16G 1008M 15G 7% /runtmpfs 16G 0 16G 0% /sys/fs/cgroup/dev/vda1 1014M 134M 881M 14% /boottmpfs 3.2G 0 3.2G 0% /run/user/0/dev/sda1 296G 188G 94G 67% /data 如果有没挂载的磁盘可以通过mount挂载 1mount /dev/sda1 /data","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"http://lvyanquan.github.io/tags/%E8%BF%90%E7%BB%B4/"},{"name":"linux","slug":"linux","permalink":"http://lvyanquan.github.io/tags/linux/"}]},{"title":"Hudi 查询加速","slug":"Hudi 查询加速","date":"2023-03-12T02:22:41.000Z","updated":"2023-03-15T02:56:30.019Z","comments":true,"path":"2023/03/12/Hudi 查询加速/","link":"","permalink":"http://lvyanquan.github.io/2023/03/12/Hudi%20%E6%9F%A5%E8%AF%A2%E5%8A%A0%E9%80%9F/","excerpt":"","text":"索引Hudi 支持多种索引： HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE, GLOBAL_SIMPLE, BUCKET。 但以下索引不建议使用： INMEMRY 索引 根据内存中的索引数据去匹配，基本不可用 SIMPLE 索引 根据新数据所在的分区，获取受影响的分区文件列表，直接读取该部分 parquet 文件的 partition_key 和 record_key，与新数据执行 leftOutJoin 产生索引数据，性能低下（以 hoodieKey 为匹配键） HBASE 索引 需要引入额外的 Hbase 服务，业务方基本不会允许 GLOBAL_SIMPLE 索引 与 SIMPLE 索引相比，GLOBAL_SIMPLE 索引读取的旧数据是全量数据，以 recordkey为匹配键，性能更低下 BLOOM写入信息org.apache.hudi.avro.HoodieBloomFilterWriteSupport#finalizeMetadata. 123456789101112131415public Map&lt;String, String&gt; finalizeMetadata() &#123; HashMap&lt;String, String&gt; extraMetadata = new HashMap&lt;&gt;(); extraMetadata.put(HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY, bloomFilter.serializeToString()); if (bloomFilter.getBloomFilterTypeCode().name().contains(HoodieDynamicBoundedBloomFilter.TYPE_CODE_PREFIX)) &#123; extraMetadata.put(HOODIE_BLOOM_FILTER_TYPE_CODE, bloomFilter.getBloomFilterTypeCode().name()); &#125; if (minRecordKey != null &amp;&amp; maxRecordKey != null) &#123; extraMetadata.put(HOODIE_MIN_RECORD_KEY_FOOTER, minRecordKey.toString()); extraMetadata.put(HOODIE_MAX_RECORD_KEY_FOOTER, maxRecordKey.toString()); &#125; return extraMetadata;&#125; 可以看到这里会写入布隆过滤器序列化后的信息和主键的 MIN&#x2F;MAX 信息。写入到 parquet 文件的 footer 中。 读取信息org.apache.hudi.common.util.BaseFileUtils#readBloomFilterFromMetadata. 12345678910111213141516171819202122public BloomFilter readBloomFilterFromMetadata(Configuration configuration, Path filePath) &#123; Map&lt;String, String&gt; footerVals = readFooter(configuration, false, filePath, HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY, HoodieAvroWriteSupport.OLD_HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY, HoodieBloomFilterWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE); String footerVal = footerVals.get(HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY); if (null == footerVal) &#123; // We use old style key &quot;com.uber.hoodie.bloomfilter&quot; footerVal = footerVals.get(HoodieAvroWriteSupport.OLD_HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY); &#125; BloomFilter toReturn = null; if (footerVal != null) &#123; if (footerVals.containsKey(HoodieBloomFilterWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE)) &#123; toReturn = BloomFilterFactory.fromString(footerVal, footerVals.get(HoodieBloomFilterWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE)); &#125; else &#123; toReturn = BloomFilterFactory.fromString(footerVal, BloomFilterTypeCode.SIMPLE.name()); &#125; &#125; return toReturn;&#125; 从 footer 中读取信息，构建布隆过滤器。 结论还是写入 footer 中，跟 Iceberg 差不多，只是 bloomfilter 的算法可能不同。 GLOBAL_BLOOM全局索引强制跨表的所有分区的键的唯一性，即保证表中对于给定的记录键恰好存在一条记录。 和 BLOOM 的区别加载文件时加载全部分区的数据文件.org.apache.hudi.index.bloom.HoodieGlobalBloomIndex#loadColumnRangesFromFiles 1234567@OverrideList&lt;Pair&lt;String, BloomIndexFileInfo&gt;&gt; loadColumnRangesFromFiles(List&lt;String&gt; partitions, final HoodieEngineContext context, final HoodieTable hoodieTable) &#123; HoodieTableMetaClient metaClient = hoodieTable.getMetaClient(); List&lt;String&gt; allPartitionPaths = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), metaClient.getBasePath()); return super.loadColumnRangesFromFiles(allPartitionPaths, context, hoodieTable);&#125; org.apache.hudi.index.bloom.HoodieBloomIndex#loadColumnRangesFromFiles.只加载特定分区 12345678910111213141516171819List&lt;Pair&lt;String, BloomIndexFileInfo&gt;&gt; loadColumnRangesFromFiles( List&lt;String&gt; partitions, final HoodieEngineContext context, final HoodieTable hoodieTable) &#123; // Obtain the latest data files from all the partitions. List&lt;Pair&lt;String, Pair&lt;String, HoodieBaseFile&gt;&gt;&gt; partitionPathFileIDList = getLatestBaseFilesForAllPartitions(partitions, context, hoodieTable).stream() .map(pair -&gt; Pair.of(pair.getKey(), Pair.of(pair.getValue().getFileId(), pair.getValue()))) .collect(toList()); context.setJobStatus(this.getClass().getName(), &quot;Obtain key ranges for file slices (range pruning=on): &quot; + config.getTableName()); return context.map(partitionPathFileIDList, pf -&gt; &#123; try &#123; HoodieRangeInfoHandle rangeInfoHandle = new HoodieRangeInfoHandle(config, hoodieTable, Pair.of(pf.getKey(), pf.getValue().getKey())); String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys(pf.getValue().getValue()); return Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue().getKey(), minMaxKeys[0], minMaxKeys[1])); &#125; catch (MetadataNotFoundException me) &#123; LOG.warn(&quot;Unable to find range metadata in file :&quot; + pf); return Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue().getKey())); &#125; &#125;, Math.max(partitionPathFileIDList.size(), 1));&#125; 结论这么看，GLOBAL_BLOOM 并不能实现比 BLOOM 更好的性能。 BUCKET推荐在较大数据量的场景下使用，避免布隆过滤器的假阳性问题。 MetadataTableMetadataTable 是一张包含元数据的 mor 表，记录了数据表的文件、列统计、布隆过滤器的信息。 写入打标： engine column_stat_idx bloom_filter_idx bucket_idx flink_state Simple Hbase_idx Spark Y Y Y N flink only Y Y Spark N N Y Y N spark only N MetaDataTable表索引分区构建： engine file_idx column_stat_idx bloom_filter_idx Spark Y Y Y Spark Y Y Y 读取data skipping： engine column_stat_idx bloom_filter_idx bucket_idx Spark Y N N Spark Y N N MetadataTable 的使用.org.apache.hudi.HoodieFileIndex#lookupCandidateFilesInMetadataTable 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * Computes pruned list of candidate base-files&#x27; names based on provided list of &#123;@link dataFilters&#125; * conditions, by leveraging Metadata Table&#x27;s Column Statistics index (hereon referred as ColStats for brevity) * bearing &quot;min&quot;, &quot;max&quot;, &quot;num_nulls&quot; statistics for all columns. * * NOTE: This method has to return complete set of candidate files, since only provided candidates will * ultimately be scanned as part of query execution. Hence, this method has to maintain the * invariant of conservatively including every base-file&#x27;s name, that is NOT referenced in its index. * * @param queryFilters list of original data filters passed down from querying engine * @return list of pruned (data-skipped) candidate base-files&#x27; names */private def lookupCandidateFilesInMetadataTable(queryFilters: Seq[Expression]): Try[Option[Set[String]]] = Try &#123; // NOTE: Data Skipping is only effective when it references columns that are indexed w/in // the Column Stats Index (CSI). Following cases could not be effectively handled by Data Skipping: // - Expressions on top-level column&#x27;s fields (ie, for ex filters like &quot;struct.field &gt; 0&quot;, since // CSI only contains stats for top-level columns, in this case for &quot;struct&quot;) // - Any expression not directly referencing top-level column (for ex, sub-queries, since there&#x27;s // nothing CSI in particular could be applied for) lazy val queryReferencedColumns = collectReferencedColumns(spark, queryFilters, schema) if (!isMetadataTableEnabled || !isDataSkippingEnabled || !columnStatsIndex.isIndexAvailable) &#123; validateConfig() Option.empty &#125; else if (queryFilters.isEmpty || queryReferencedColumns.isEmpty) &#123; Option.empty &#125; else &#123; // NOTE: Since executing on-cluster via Spark API has its own non-trivial amount of overhead, // it&#x27;s most often preferential to fetch Column Stats Index w/in the same process (usually driver), // w/o resorting to on-cluster execution. // For that we use a simple-heuristic to determine whether we should read and process CSI in-memory or // on-cluster: total number of rows of the expected projected portion of the index has to be below the // threshold (of 100k records) val shouldReadInMemory = columnStatsIndex.shouldReadInMemory(this, queryReferencedColumns) columnStatsIndex.loadTransposed(queryReferencedColumns, shouldReadInMemory) &#123; transposedColStatsDF =&gt; val indexSchema = transposedColStatsDF.schema val indexFilter = queryFilters.map(translateIntoColumnStatsIndexFilterExpr(_, indexSchema)) .reduce(And) val allIndexedFileNames = transposedColStatsDF.select(HoodieMetadataPayload.COLUMN_STATS_FIELD_FILE_NAME) .collect() .map(_.getString(0)) .toSet val prunedCandidateFileNames = transposedColStatsDF.where(new Column(indexFilter)) .select(HoodieMetadataPayload.COLUMN_STATS_FIELD_FILE_NAME) .collect() .map(_.getString(0)) .toSet // NOTE: Col-Stats Index isn&#x27;t guaranteed to have complete set of statistics for every // base-file: since it&#x27;s bound to clustering, which could occur asynchronously // at arbitrary point in time, and is not likely to be touching all of the base files. // // To close that gap, we manually compute the difference b/w all indexed (by col-stats-index) // files and all outstanding base-files, and make sure that all base files not // represented w/in the index are included in the output of this method val notIndexedFileNames = lookupFileNamesMissingFromIndex(allIndexedFileNames) Some(prunedCandidateFileNames ++ notIndexedFileNames) &#125; &#125; &#125; 结论dataskip 的时候只使用了 column_stat_idx 信息。 其他优化TimelineService构建 timeline 的时候避免从 HDFS 上读取数据文件。","categories":[],"tags":[{"name":"数据湖","slug":"数据湖","permalink":"http://lvyanquan.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B9%96/"}]},{"title":"一致性哈希","slug":"一致性哈希","date":"2023-03-12T02:22:41.000Z","updated":"2023-03-12T06:48:04.410Z","comments":true,"path":"2023/03/12/一致性哈希/","link":"","permalink":"http://lvyanquan.github.io/2023/03/12/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/","excerpt":"","text":"背景一致性哈希解决的是普通哈希水平拓展时需要全部迁移数据的问题。 原理 环形空间 映射服务器节点 映射数据","categories":[],"tags":[]},{"title":"两阶段提交","slug":"两阶段提交","date":"2023-03-12T02:22:41.000Z","updated":"2023-04-26T09:16:57.053Z","comments":true,"path":"2023/03/12/两阶段提交/","link":"","permalink":"http://lvyanquan.github.io/2023/03/12/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/","excerpt":"","text":"分布式事务分布式事务是指跨越多个计算机或者多个网络节点的事务。在分布式系统中，一个事务可能需要访问和修改多个节点上的数据，而且这些节点可能分布在不同的物理位置上。由于存在多个节点，因此需要确保事务的原子性、一致性、隔离性和持久性，这些性质被称为 ACID 属性。 分布式事务需要通过协调器（coordinator）来实现。协调器负责协调所有参与者节点的操作，确保所有节点都遵循相同的事务处理流程，并最终提交或回滚事务。协调器需要处理节点故障、网络分区等复杂情况，保证分布式事务的正确性和可靠性。 分布式事务的实现方式包括两阶段提交协议（Two-Phase Commit，2PC）、三阶段提交协议（Three-Phase Commit，3PC）、Paxos 协议等。这些协议各有优缺点，应根据具体情况进行选择。 两阶段提交两阶段提交（Two-Phase Commit，2PC）是一种用于实现分布式事务的协议。它由一个协调者（coordinator）和多个参与者（participant）组成。 在 2PC 协议中，事务分为两个阶段：准备阶段（Prepare Phase）和提交阶段（Commit Phase）。 准备阶段在准备阶段，协调者向所有参与者发送请求，要求参与者准备好执行该事务，并将准备结果通知给协调者。参与者在接收到请求后，会执行本地的事务操作，并将执行结果返回给协调者。在这个阶段，如果有任何一个参与者无法正常响应，则协调者会向所有参与者发送回滚请求，取消该事务的执行。 提交阶段如果所有参与者都已经准备好执行该事务，并且协调者已经确认了所有参与者的准备结果，那么事务就进入提交阶段。在这个阶段，协调者向所有参与者发送提交请求，要求参与者执行该事务，并将执行结果通知给协调者。如果所有参与者都成功地执行了该事务，那么协调者就向所有参与者发送提交完成的消息，表示该事务已经成功提交。如果在提交阶段中任何一个参与者无法执行该事务，那么协调者就向所有参与者发送回滚请求，取消该事务的执行。 2PC 协议的优点是简单、可靠。它可以确保所有参与者要么全部执行该事务，要么全部取消该事务，从而保证了分布式事务的 ACID 特性。然而，2PC 协议也存在一些缺点，例如： 参与者在等待协调者的通知时，可能会一直处于阻塞状态，影响系统的吞吐量。 协调者在发生故障时，可能会导致整个系统无法正常工作。 2PC 协议的执行效率较低，因为需要进行两个阶段的通信和协调。","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://lvyanquan.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"大数据","slug":"大数据","permalink":"http://lvyanquan.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"面试题 17.05. 字母与数字","slug":"面试题 17.05. 字母与数字","date":"2023-03-12T02:22:41.000Z","updated":"2023-04-26T09:17:26.195Z","comments":true,"path":"2023/03/12/面试题 17.05. 字母与数字/","link":"","permalink":"http://lvyanquan.github.io/2023/03/12/%E9%9D%A2%E8%AF%95%E9%A2%98%2017.05.%20%E5%AD%97%E6%AF%8D%E4%B8%8E%E6%95%B0%E5%AD%97/","excerpt":"","text":"面试题 17.05. 字母与数字 题目给定一个放有字母和数字的数组，找到最长的子数组，且包含的字母和数字的个数相同。 返回该子数组，若存在多个最长子数组，返回左端点下标值最小的子数组。若不存在这样的数组，返回一个空数组。 示例 1: 输入: [“A”,”1”,”B”,”C”,”D”,”2”,”3”,”4”,”E”,”5”,”F”,”G”,”6”,”7”,”H”,”I”,”J”,”K”,”L”,”M”] 输出: [“A”,”1”,”B”,”C”,”D”,”2”,”3”,”4”,”E”,”5”,”F”,”G”,”6”,”7”] 示例 2: 输入: [“A”,”A”] 输出: [] 提示： array.length &lt;&#x3D; 100000 思路 看到 100000 这个数量就知道不能用 n2 的复杂度了，而且这题没有明显的二分特点，所以只能考虑 n 的时间复杂度，那么只能考虑使用哈希表降低搜索范围了。 但是哈希表里应该保存什么呢？因为我们要求的是子数组里数字和字符数量相同，那么在枚举右端点的时候，我们需要快速找到能够让区间满足要求的左端点。这引导我们考虑采用前缀和的方式做处理，只要右端点的“数字出现的次数 - 字符串出现的次数”的差值等于左端点就可以满足要求了。前缀和的计算方式：1key += array[i].charAt(0) &gt;= &#x27;0&#x27; &amp;&amp; array[i].charAt(0) &lt;= &#x27;9&#x27; ? 1 : -1; 因为题目要求在区间长度相同时返回最左区间，所以哈希表的记录的是键为“数字出现的次数 - 字符串出现的次数”，值为这个键出现最早的位置。 代码12345678910111213141516171819202122class Solution &#123; public String[] findLongestSubarray(String[] array) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); int key = 0; int left = 0, len = 0; map.put(0, -1); for(int i=0;i&lt;array.length;i++) &#123; key += array[i].charAt(0) &gt;= &#x27;0&#x27; &amp;&amp; array[i].charAt(0) &lt;= &#x27;9&#x27; ? 1 : -1; if(map.containsKey(key)) &#123; int pos = map.get(key); if( (len &lt; i - pos) || ((len == i - pos) &amp;&amp; pos &lt; left) )&#123; len = i - pos; left = pos + 1; &#125; &#125;else&#123; map.put(key, i); &#125; &#125; return Arrays.copyOfRange(array, left, left + len); &#125;&#125;","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"http://lvyanquan.github.io/tags/%E7%AE%97%E6%B3%95/"}]}],"categories":[],"tags":[{"name":"Hudi","slug":"Hudi","permalink":"http://lvyanquan.github.io/tags/Hudi/"},{"name":"bugfix","slug":"bugfix","permalink":"http://lvyanquan.github.io/tags/bugfix/"},{"name":"Java","slug":"Java","permalink":"http://lvyanquan.github.io/tags/Java/"},{"name":"juc","slug":"juc","permalink":"http://lvyanquan.github.io/tags/juc/"},{"name":"FlinkCDC","slug":"FlinkCDC","permalink":"http://lvyanquan.github.io/tags/FlinkCDC/"},{"name":"算法","slug":"算法","permalink":"http://lvyanquan.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"运维","slug":"运维","permalink":"http://lvyanquan.github.io/tags/%E8%BF%90%E7%BB%B4/"},{"name":"linux","slug":"linux","permalink":"http://lvyanquan.github.io/tags/linux/"},{"name":"数据湖","slug":"数据湖","permalink":"http://lvyanquan.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B9%96/"},{"name":"分布式","slug":"分布式","permalink":"http://lvyanquan.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"大数据","slug":"大数据","permalink":"http://lvyanquan.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}