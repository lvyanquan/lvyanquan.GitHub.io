<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://lvyanquan.github.io/atom.xml" rel="self"/>
  
  <link href="http://lvyanquan.github.io/"/>
  <updated>2023-05-19T09:41:24.282Z</updated>
  <id>http://lvyanquan.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hudi cdc源码分析</title>
    <link href="http://lvyanquan.github.io/2023/05/19/Hudi%20cdc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>http://lvyanquan.github.io/2023/05/19/Hudi%20cdc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</id>
    <published>2023-05-19T09:38:10.560Z</published>
    <updated>2023-05-19T09:41:24.282Z</updated>
    
    <content type="html"><![CDATA[<p>原理部分参考<a href="https://developer.aliyun.com/article/1164177">这篇博客</a></p><p>HoodieMergeHandleFactory 在开启cdc时创建HoodieMergeHandleWithChangeLog：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Creates a merge handle for normal write path.</span><br><span class="line"> */</span><br><span class="line">public static &lt;T, I, K, O&gt; HoodieMergeHandle&lt;T, I, K, O&gt; create(</span><br><span class="line">    WriteOperationType operationType,</span><br><span class="line">    HoodieWriteConfig writeConfig,</span><br><span class="line">    String instantTime,</span><br><span class="line">    HoodieTable&lt;T, I, K, O&gt; table,</span><br><span class="line">    Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr,</span><br><span class="line">    String partitionPath,</span><br><span class="line">    String fileId,</span><br><span class="line">    TaskContextSupplier taskContextSupplier,</span><br><span class="line">    Option&lt;BaseKeyGenerator&gt; keyGeneratorOpt) &#123;</span><br><span class="line">  if (table.requireSortedRecords()) &#123;</span><br><span class="line">    if (table.getMetaClient().getTableConfig().isCDCEnabled()) &#123;</span><br><span class="line">      return new HoodieSortedMergeHandleWithChangeLog&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier,</span><br><span class="line">          keyGeneratorOpt);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return new HoodieSortedMergeHandle&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier,</span><br><span class="line">          keyGeneratorOpt);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else if (!WriteOperationType.isChangingRecords(operationType) &amp;&amp; writeConfig.allowDuplicateInserts()) &#123;</span><br><span class="line">    return new HoodieConcatHandle&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    if (table.getMetaClient().getTableConfig().isCDCEnabled()) &#123;</span><br><span class="line">      return new HoodieMergeHandleWithChangeLog&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return new HoodieMergeHandle&lt;&gt;(writeConfig, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>HoodieMergeHandleWithChangeLog 初始化时会创建一个HoodieCDCLogger对象。<br>在update和insert数据时写入变更数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">protected boolean writeUpdateRecord(HoodieRecord&lt;T&gt; newRecord, HoodieRecord&lt;T&gt; oldRecord, Option&lt;HoodieRecord&gt; combinedRecordOpt, Schema writerSchema)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line">  // TODO [HUDI-5019] Remove these unnecessary newInstance invocations</span><br><span class="line">  Option&lt;HoodieRecord&gt; savedCombineRecordOp = combinedRecordOpt.map(HoodieRecord::newInstance);</span><br><span class="line">  final boolean result = super.writeUpdateRecord(newRecord, oldRecord, combinedRecordOpt, writerSchema);</span><br><span class="line">  if (result) &#123;</span><br><span class="line">    boolean isDelete = HoodieOperation.isDelete(newRecord.getOperation());</span><br><span class="line">    Option&lt;IndexedRecord&gt; avroRecordOpt = savedCombineRecordOp.flatMap(r -&gt;</span><br><span class="line">        toAvroRecord(r, writerSchema, config.getPayloadConfig().getProps()));</span><br><span class="line">    cdcLogger.put(newRecord, (GenericRecord) oldRecord.getData(), isDelete ? Option.empty() : avroRecordOpt);</span><br><span class="line">  &#125;</span><br><span class="line">  return result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected void writeInsertRecord(HoodieRecord&lt;T&gt; newRecord) throws IOException &#123;</span><br><span class="line">  Schema schema = useWriterSchemaForCompaction ? writeSchemaWithMetaFields : writeSchema;</span><br><span class="line">  // TODO Remove these unnecessary newInstance invocations</span><br><span class="line">  HoodieRecord&lt;T&gt; savedRecord = newRecord.newInstance();</span><br><span class="line">  super.writeInsertRecord(newRecord);</span><br><span class="line">  if (!HoodieOperation.isDelete(newRecord.getOperation())) &#123;</span><br><span class="line">    cdcLogger.put(newRecord, null, savedRecord.toIndexedRecord(schema, config.getPayloadConfig().getProps()).map(HoodieAvroIndexedRecord::getData));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在insert数据的情况下，oldRecord写入null。<br>HoodieCDCLogger 通过比较oldRecord和newRecord判断这条数据的操作类型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public void put(HoodieRecord hoodieRecord,</span><br><span class="line">                GenericRecord oldRecord,</span><br><span class="line">                Option&lt;IndexedRecord&gt; newRecord) &#123;</span><br><span class="line">  String recordKey = hoodieRecord.getRecordKey();</span><br><span class="line">  GenericData.Record cdcRecord;</span><br><span class="line">  if (newRecord.isPresent()) &#123;</span><br><span class="line">    GenericRecord record = (GenericRecord) newRecord.get();</span><br><span class="line">    if (oldRecord == null) &#123;</span><br><span class="line">      // INSERT cdc record</span><br><span class="line">      cdcRecord = this.transformer.transform(HoodieCDCOperation.INSERT, recordKey,</span><br><span class="line">          null, record);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // UPDATE cdc record</span><br><span class="line">      cdcRecord = this.transformer.transform(HoodieCDCOperation.UPDATE, recordKey,</span><br><span class="line">          oldRecord, record);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // DELETE cdc record</span><br><span class="line">    cdcRecord = this.transformer.transform(HoodieCDCOperation.DELETE, recordKey,</span><br><span class="line">        oldRecord, null);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  flushIfNeeded(false);</span><br><span class="line">  HoodieAvroPayload payload = new HoodieAvroPayload(Option.of(cdcRecord));</span><br><span class="line">  if (cdcData.isEmpty()) &#123;</span><br><span class="line">    averageCDCRecordSize = sizeEstimator.sizeEstimate(payload);</span><br><span class="line">  &#125;</span><br><span class="line">  cdcData.put(recordKey, payload);</span><br><span class="line">  numOfCDCRecordsInMemory.incrementAndGet();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原理部分参考&lt;a href=&quot;https://developer.aliyun.com/article/1164177&quot;&gt;这篇博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;HoodieMergeHandleFactory 在开启cdc时创建HoodieMergeHandleWithChang</summary>
      
    
    
    
    
    <category term="Hudi" scheme="http://lvyanquan.github.io/tags/Hudi/"/>
    
  </entry>
  
  <entry>
    <title>Hudi merge into多条件更新</title>
    <link href="http://lvyanquan.github.io/2023/05/17/Hudi%20merge%20into%E5%A4%9A%E6%9D%A1%E4%BB%B6%E6%9B%B4%E6%96%B0/"/>
    <id>http://lvyanquan.github.io/2023/05/17/Hudi%20merge%20into%E5%A4%9A%E6%9D%A1%E4%BB%B6%E6%9B%B4%E6%96%B0/</id>
    <published>2023-05-17T05:50:09.575Z</published>
    <updated>2023-05-17T05:50:09.626Z</updated>
    
    <content type="html"><![CDATA[<p>验证这个<a href="https://issues.apache.org/jira/browse/HUDI-5904">jira</a></p><p>创建表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">create table base (</span><br><span class="line">  id int,</span><br><span class="line">  price int</span><br><span class="line">) using hudi</span><br><span class="line">tblproperties (</span><br><span class="line">  primaryKey = &#x27;id&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">create table change (</span><br><span class="line">  id int,</span><br><span class="line">  price int</span><br><span class="line">) using hudi</span><br><span class="line">tblproperties (</span><br><span class="line">  primaryKey = &#x27;id&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>插入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">insert into base values (1, 1), (2, 2);</span><br><span class="line">insert into change values (1, 10), (2, 20), (3, 30);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行按条件更新(设置多种条件)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">merge into base as target</span><br><span class="line">using (select id, price from change) source</span><br><span class="line">on target.id = source.id</span><br><span class="line">/* 条件一 id=1时相加 */</span><br><span class="line">when matched and target.id = 1 then</span><br><span class="line">update set target.price = source.price + target.price</span><br><span class="line">/* 条件二 设置为较大的price */</span><br><span class="line">when matched and target.price &lt; source.price then</span><br><span class="line">update set target.price = source.price</span><br><span class="line">/* 不匹配时插入 */</span><br><span class="line">when not matched then</span><br><span class="line">insert *;</span><br></pre></td></tr></table></figure><p>查询base表数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">20230517133958447       20230517133958447_0_0   2               c4be3a67-b9f4-4593-883f-9cca0f1e1e15-0_0-4-4_20230517133958447.parquet      2       20</span><br><span class="line">20230517133958447       20230517133958447_0_1   1               c4be3a67-b9f4-4593-883f-9cca0f1e1e15-0_0-4-4_20230517133958447.parquet      1       11</span><br><span class="line">20230517133958447       20230517133958447_0_2   3               c4be3a67-b9f4-4593-883f-9cca0f1e1e15-0_0-4-4_20230517133958447.parquet      3       30</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;验证这个&lt;a href=&quot;https://issues.apache.org/jira/browse/HUDI-5904&quot;&gt;jira&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建表&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td </summary>
      
    
    
    
    
    <category term="Hudi" scheme="http://lvyanquan.github.io/tags/Hudi/"/>
    
  </entry>
  
  <entry>
    <title>使用anki构建个人单词学习库</title>
    <link href="http://lvyanquan.github.io/2023/05/12/%E4%BD%BF%E7%94%A8anki%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%95%E8%AF%8D%E5%AD%A6%E4%B9%A0%E5%BA%93/"/>
    <id>http://lvyanquan.github.io/2023/05/12/%E4%BD%BF%E7%94%A8anki%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%95%E8%AF%8D%E5%AD%A6%E4%B9%A0%E5%BA%93/</id>
    <published>2023-05-12T08:37:54.842Z</published>
    <updated>2023-05-12T10:05:43.738Z</updated>
    
    <content type="html"><![CDATA[<p>过去学习英语都是面向考试学习的，四六级、考研、雅思等，都有具体的单词书。但是在工作中，需要看的英语文档都是专业方向的，与通用的英语单词本重合度比较低。于是我找到了anki这么一个可以自定义单词内容的学习软件，这里把使用流程记录一下。</p><h3 id="安装anki"><a href="#安装anki" class="headerlink" title="安装anki"></a>安装anki</h3><p>通过<a href="https://apps.ankiweb.net/">Anki官网</a>，点击Download按钮跳转至页面底部。<br>安装流程还是很简单的，安装好后的界面如下：<br><img src="/images/3.png"></p><p>当然，这里的“英语单词”牌照是我自己添加的。<br>简单介绍一下这个软件里的术语：<br>卡片：卡片就是你设计的一个知识点，可以是单词，也可以是一首诗，自由定义。<br>牌组：牌组就是卡片的集合，也就是知识库了。<br>卡片模板：对于一个知识点，你想要以什么样的方式呈现，这可以通过设计模板实现。<br>以基础的记单词模型为例，我们会给出单词的英文，思考记忆单词的含义，这就是问答题的模板，我们可以在“添加”这一栏里找到这个配置模板：<br><img src="/images/4.png"><br>并且根据我们的知识点填上正面和反面需要展示的内容：<br><img src="/images/5.png"><br>这样，一张卡片就完成了，当然，你可以需要预先创建一个知识库牌组。</p><h3 id="安装浏览器插件"><a href="#安装浏览器插件" class="headerlink" title="安装浏览器插件"></a>安装浏览器插件</h3><p>但是，每次都自己输入单词的英文和中文释义也还是有些麻烦，鉴于我主要是在浏览器上查看英文文档，找了相关的浏览器插件支持一下。<br>这里推荐的是<a href="https://chrome.google.com/webstore/detail/online-dictionary-helper/lppjdajkacanlmpbbcdkccjkdbpllajb?hl=zh-CN">在线词典助手</a>,点击安装就可以了。<br>安装好这个拓展以后，我们就要把这个浏览器插件跟anki绑定起来了，这里通过ankiconnect绑定，ankiconnect是一个插件，可以直接通过anki安装。<br>在“工具”-“插件”-“获取插件”这里可以找到下载插件的地方，输入插件编号“2055492159”自动下载ankiconnect：<br><img src="/images/6.png"><br>然后去在线词典助手上配置绑定信息，以问答题为例，配置输出选项，选择单词词组在正面，单一释义在背面。<br><img src="/images/7.png"><br>当然，为了更好的应用辞典信息，只有英文单词和中文含义是不够的，所以最好还是要修改一下模板。<br>这里选用这个<a href="https://www.laohuang.net/files/ODH.zip">模板</a><br><img src="/images/8.png"></p><h3 id="开始学习"><a href="#开始学习" class="headerlink" title="开始学习"></a>开始学习</h3><p>配置好浏览器插件后，对自己想要学习的单词点击鼠标右键，点选“➕”号把单词添加到牌组里。<br><img src="/images/9.png"><br>点击牌组，开始学习这个单词。<br>正面：<br><img src="/images/10.png"><br>背面：<br><img src="/images/11.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;过去学习英语都是面向考试学习的，四六级、考研、雅思等，都有具体的单词书。但是在工作中，需要看的英语文档都是专业方向的，与通用的英语单词本重合度比较低。于是我找到了anki这么一个可以自定义单词内容的学习软件，这里把使用流程记录一下。&lt;/p&gt;
&lt;h3 id=&quot;安装anki&quot;&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>LinkageError解决</title>
    <link href="http://lvyanquan.github.io/2023/05/10/LinkageError%E8%A7%A3%E5%86%B3/"/>
    <id>http://lvyanquan.github.io/2023/05/10/LinkageError%E8%A7%A3%E5%86%B3/</id>
    <published>2023-05-10T11:35:42.700Z</published>
    <updated>2023-05-12T06:37:42.889Z</updated>
    
    <content type="html"><![CDATA[<p>思路参考这个<a href="https://stackoverflow.com/questions/244482/how-to-deal-with-linkageerrors-in-java">回答</a><br>在解决依赖冲突的场景中，偶尔会遇到这个问题：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java.lang.LinkageError: loader constraint violation: when resolving method &quot;org.slf4j.impl.StaticLoggerBinder.getLoggerFactory()Lorg/slf4j/ILoggerFactory;&quot; the class loader (instance of org/apache/catalina/loader/WebappClassLoader) of the current class, org/slf4j/LoggerFactory, and the class loader (instance of org/apache/catalina/loader/StandardClassLoader) for resolved class, org/slf4j/impl/StaticLoggerBinder, have different Class objects for the type taticLoggerBinder.getLoggerFactory()Lorg/slf4j/ILoggerFactory; used in the signature</span><br><span class="line">at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:299)</span><br><span class="line">at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:269)</span><br><span class="line">at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)</span><br></pre></td></tr></table></figure><p>显然，这是log4j的依赖冲突，但是通过idea的Mavendependency Analyzer并没有找到冲突的依赖，想办法排依赖可能就解决了。然而这种解决方式并不靠谱。这里把问题出现的原因和解决流程记录一下。</p><h3 id="排查流程"><a href="#排查流程" class="headerlink" title="排查流程"></a>排查流程</h3><p>这个问题是ILoggerFactory这个类被加载了两次</p><ol><li>在jvm参数里添加 -verbose:class， 打印类的加载路径<br>找到冲突的jar包<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Loaded org.slf4j.ILoggerFactory from file:/Users/kunni/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar]</span><br><span class="line">......</span><br><span class="line">[Loaded org.slf4j.ILoggerFactory from file:/Users/kunni/.m2/repository/org/apache/hive/hive-exec/2.3.8/hive-exec-2.3.8.jar]</span><br></pre></td></tr></table></figure>可以看到这个类在slf4j-api-1.7.30.jar和hive-exec-2.3.8.jar上都出现了，可以断定hive-exec-2.3.8.jar里包含了其他版本的slf4j依赖。</li><li>通过反编译查看依赖版本<br>通过jd-gui查看hive-exec-2.3.8.jar里包含的slf4j依赖版本<br><img src="/images/2.png"><br>发现两个版本确实是不一致的，想办法解决这个问题。</li></ol><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><ol><li>通过maven的依赖覆盖解决这个问题</li><li>通过maven shade插件改名解决这个问题<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.2.4&lt;/version&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;artifactSet&gt;</span><br><span class="line">                    &lt;includes&gt;</span><br><span class="line">                        &lt;include&gt;*:*&lt;/include&gt;</span><br><span class="line">                    &lt;/includes&gt;</span><br><span class="line">                &lt;/artifactSet&gt;</span><br><span class="line">                &lt;relocations&gt;</span><br><span class="line">                    &lt;relocation&gt;</span><br><span class="line">                        &lt;pattern&gt;org.slf4j..&lt;/pattern&gt;</span><br><span class="line">                        &lt;shadedPattern&gt;shade.org.slf4j.&lt;/shadedPattern&gt;</span><br><span class="line">                    &lt;/relocation&gt;</span><br><span class="line">                &lt;/relocations&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;思路参考这个&lt;a href=&quot;https://stackoverflow.com/questions/244482/how-to-deal-with-linkageerrors-in-java&quot;&gt;回答&lt;/a&gt;&lt;br&gt;在解决依赖冲突的场景中，偶尔会遇到这个问题：&lt;/p&gt;
&lt;f</summary>
      
    
    
    
    
    <category term="bugfix" scheme="http://lvyanquan.github.io/tags/bugfix/"/>
    
    <category term="Java" scheme="http://lvyanquan.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>volatile关键字</title>
    <link href="http://lvyanquan.github.io/2023/05/09/volatile%E5%85%B3%E9%94%AE%E5%AD%97/"/>
    <id>http://lvyanquan.github.io/2023/05/09/volatile%E5%85%B3%E9%94%AE%E5%AD%97/</id>
    <published>2023-05-09T11:30:50.219Z</published>
    <updated>2023-05-10T02:01:58.686Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.v2ex.com/t/938630">来源</a><br>有序性指的是程序按照代码的先后顺序执行。而编译器为了优化性能，有时候会改变程序中语句的先后顺序。<br>Java 中经典的案例就是利用双重检查创建单例对象，其中 volatile 就是保证有序性的。   </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class Singleton &#123;</span><br><span class="line">    private static volatile Singleton singleton;</span><br><span class="line">    private Singleton() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    public static Singleton getInstance() &#123;</span><br><span class="line">        if (singleton == null) &#123;</span><br><span class="line">            synchronized (Singleton.class) &#123;</span><br><span class="line">                if (singleton == null) &#123;</span><br><span class="line">                    singleton = new Singleton();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return singleton;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果没有 volatile ，我们以为的 new 操作应该是： </p><ol><li>分配一块内存 M ；</li><li>在内存 M 上初始化 Singleton 对象；</li><li>然后 M 的地址赋值给 instance 变量。</li></ol><p>但是实际上优化后的执行路径却是这样的：  </p><ol><li>分配一块内存 M ；</li><li>将 M 的地址赋值给 instance 变量；</li><li>最后在内存 M 上初始化 Singleton 对象。</li></ol><p>假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance !&#x3D; null ，所以直接返回 instance ，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。<br><img src="/images/1.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.v2ex.com/t/938630&quot;&gt;来源&lt;/a&gt;&lt;br&gt;有序性指的是程序按照代码的先后顺序执行。而编译器为了优化性能，有时候会改变程序中语句的先后顺序。&lt;br&gt;Java 中经典的案例就是利用双重检查创建单例对象，其中 volati</summary>
      
    
    
    
    
    <category term="Java" scheme="http://lvyanquan.github.io/tags/Java/"/>
    
    <category term="juc" scheme="http://lvyanquan.github.io/tags/juc/"/>
    
  </entry>
  
  <entry>
    <title>FlinkCDC全增量读取源码分析</title>
    <link href="http://lvyanquan.github.io/2023/05/06/FlinkCDC%E5%85%A8%E5%A2%9E%E9%87%8F%E8%AF%BB%E5%8F%96%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>http://lvyanquan.github.io/2023/05/06/FlinkCDC%E5%85%A8%E5%A2%9E%E9%87%8F%E8%AF%BB%E5%8F%96%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</id>
    <published>2023-05-06T11:36:30.312Z</published>
    <updated>2023-05-10T02:43:02.434Z</updated>
    
    <content type="html"><![CDATA[<p>[未完待续]<br>快照读取的逻辑:</p><p>SHOW MASTER STATUS 获取 lw，插入队列<br>读取该分片内的记录，插入队列<br>SHOW MASTER STATUS 获取 hw，插入队列<br>判断 lw 与 hw 之间是否有增量变更<br>如果没有变更，队列中插入 BINLOG_END 记录<br>否则读取 [lw, hw] 之间的 binlog 并插入队列，最后一条记录为 BINLOG_END<br>MySqlSnapshotSplitReadTask#doExecute</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">protected SnapshotResult doExecute(</span><br><span class="line">        ChangeEventSourceContext context,</span><br><span class="line">        OffsetContext previousOffset,</span><br><span class="line">        SnapshotContext snapshotContext,</span><br><span class="line">        SnapshottingTask snapshottingTask)</span><br><span class="line">        throws Exception &#123;</span><br><span class="line">    final RelationalSnapshotChangeEventSource.RelationalSnapshotContext ctx =</span><br><span class="line">            (RelationalSnapshotChangeEventSource.RelationalSnapshotContext) snapshotContext;</span><br><span class="line">    ctx.offset = offsetContext;</span><br><span class="line"></span><br><span class="line">    final BinlogOffset lowWatermark = currentBinlogOffset(jdbcConnection);</span><br><span class="line">    LOG.info(</span><br><span class="line">            &quot;Snapshot step 1 - Determining low watermark &#123;&#125; for split &#123;&#125;&quot;,</span><br><span class="line">            lowWatermark,</span><br><span class="line">            snapshotSplit);</span><br><span class="line">    ((SnapshotSplitChangeEventSourceContext) (context)).setLowWatermark(lowWatermark);</span><br><span class="line">    dispatcher.dispatchWatermarkEvent(</span><br><span class="line">            offsetContext.getPartition(), snapshotSplit, lowWatermark, WatermarkKind.LOW);</span><br><span class="line"></span><br><span class="line">    LOG.info(&quot;Snapshot step 2 - Snapshotting data&quot;);</span><br><span class="line">    createDataEvents(ctx, snapshotSplit.getTableId());</span><br><span class="line"></span><br><span class="line">    final BinlogOffset highWatermark = currentBinlogOffset(jdbcConnection);</span><br><span class="line">    LOG.info(</span><br><span class="line">            &quot;Snapshot step 3 - Determining high watermark &#123;&#125; for split &#123;&#125;&quot;,</span><br><span class="line">            highWatermark,</span><br><span class="line">            snapshotSplit);</span><br><span class="line">    ((SnapshotSplitChangeEventSourceContext) (context)).setHighWatermark(lowWatermark);</span><br><span class="line">    dispatcher.dispatchWatermarkEvent(</span><br><span class="line">            offsetContext.getPartition(), snapshotSplit, highWatermark, WatermarkKind.HIGH);</span><br><span class="line">    return SnapshotResult.completed(ctx.offset);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[未完待续]&lt;br&gt;快照读取的逻辑:&lt;/p&gt;
&lt;p&gt;SHOW MASTER STATUS 获取 lw，插入队列&lt;br&gt;读取该分片内的记录，插入队列&lt;br&gt;SHOW MASTER STATUS 获取 hw，插入队列&lt;br&gt;判断 lw 与 hw 之间是否有增量变更&lt;br&gt;如果没</summary>
      
    
    
    
    
    <category term="FlinkCDC" scheme="http://lvyanquan.github.io/tags/FlinkCDC/"/>
    
  </entry>
  
  <entry>
    <title>2418. 按身高排序</title>
    <link href="http://lvyanquan.github.io/2023/04/25/2418.%20%E6%8C%89%E8%BA%AB%E9%AB%98%E6%8E%92%E5%BA%8F/"/>
    <id>http://lvyanquan.github.io/2023/04/25/2418.%20%E6%8C%89%E8%BA%AB%E9%AB%98%E6%8E%92%E5%BA%8F/</id>
    <published>2023-04-25T15:02:28.597Z</published>
    <updated>2023-04-26T09:16:37.422Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://leetcode.cn/problems/sort-the-people/description/">2418. 按身高排序</a></p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给你一个字符串数组 names ，和一个由 互不相同 的正整数组成的数组 heights 。两个数组的长度均为 n 。</p><p>对于每个下标 i，names[i] 和 heights[i] 表示第 i 个人的名字和身高。</p><p>请按身高 降序 顺序返回对应的名字数组 names 。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>简单题<br>稍微需要设计的是怎么将 name 和 height 绑定在一起。<br>这里使用的是 Class，也可以使用 map</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String[] sortPeople(String[] names, <span class="type">int</span>[] heights) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> heights.length;</span><br><span class="line">        Pair[] pairs = <span class="keyword">new</span> <span class="title class_">Pair</span>[n];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++) &#123;</span><br><span class="line">            pairs[i] = <span class="keyword">new</span> <span class="title class_">Pair</span>(names[i], heights[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        Arrays.sort(pairs, (Pair a, Pair b) -&gt; &#123;</span><br><span class="line">            <span class="keyword">return</span> Integer.compare(b.height, a.height);</span><br><span class="line">        &#125;);</span><br><span class="line">        String[] res = <span class="keyword">new</span> <span class="title class_">String</span>[n];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++) &#123;</span><br><span class="line">            res[i] = pairs[i].name;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">class</span> <span class="title class_">Pair</span> &#123;</span><br><span class="line">     String name;</span><br><span class="line">     <span class="type">int</span> height;</span><br><span class="line">     Pair(String name, <span class="type">int</span> height) &#123;</span><br><span class="line">         <span class="built_in">this</span>.name = name;</span><br><span class="line">         <span class="built_in">this</span>.height = height;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://leetcode.cn/problems/sort-the-people/description/&quot;&gt;2418. 按身高排序&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    
    <category term="算法" scheme="http://lvyanquan.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>linux 磁盘空间查看</title>
    <link href="http://lvyanquan.github.io/2023/03/15/linux%20%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E6%9F%A5%E7%9C%8B/"/>
    <id>http://lvyanquan.github.io/2023/03/15/linux%20%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E6%9F%A5%E7%9C%8B/</id>
    <published>2023-03-15T02:53:31.010Z</published>
    <updated>2023-04-26T09:17:30.497Z</updated>
    
    <content type="html"><![CDATA[<p>查看某个分区下文件的存储情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">du --max-dep=1 -h /</span><br><span class="line"></span><br><span class="line">示例：</span><br><span class="line">du --max-dep=1 -h /data/hadoop_admin/nm-local-dir/</span><br><span class="line">38G     /data/hadoop_admin/nm-local-dir/usercache</span><br><span class="line">56K     /data/hadoop_admin/nm-local-dir/nmPrivate</span><br><span class="line">5.6M    /data/hadoop_admin/nm-local-dir/filecache</span><br><span class="line">38G     /data/hadoop_admin/nm-local-dir/</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看磁盘使用情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> df -h</span><br><span class="line">文件系统        容量  已用  可用 已用% 挂载点</span><br><span class="line">/dev/vda3       116G   39G   77G   34% /</span><br><span class="line">devtmpfs         16G     0   16G    0% /dev</span><br><span class="line">tmpfs            16G     0   16G    0% /dev/shm</span><br><span class="line">tmpfs            16G 1008M   15G    7% /run</span><br><span class="line">tmpfs            16G     0   16G    0% /sys/fs/cgroup</span><br><span class="line">/dev/vda1      1014M  134M  881M   14% /boot</span><br><span class="line">tmpfs           3.2G     0  3.2G    0% /run/user/0</span><br><span class="line">/dev/sda1       296G  188G   94G   67% /data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果有没挂载的磁盘可以通过mount挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/sda1 /data</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;查看某个分区下文件的存储情况&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2</summary>
      
    
    
    
    
    <category term="运维" scheme="http://lvyanquan.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
    <category term="linux" scheme="http://lvyanquan.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Hudi 查询加速</title>
    <link href="http://lvyanquan.github.io/2023/03/12/Hudi%20%E6%9F%A5%E8%AF%A2%E5%8A%A0%E9%80%9F/"/>
    <id>http://lvyanquan.github.io/2023/03/12/Hudi%20%E6%9F%A5%E8%AF%A2%E5%8A%A0%E9%80%9F/</id>
    <published>2023-03-12T02:22:41.000Z</published>
    <updated>2023-03-15T02:56:30.019Z</updated>
    
    <content type="html"><![CDATA[<h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>Hudi 支持多种索引：    </p><p>HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE, GLOBAL_SIMPLE, BUCKET。   </p><p>但以下索引不建议使用：</p><ul><li>INMEMRY 索引</li></ul><p>根据内存中的索引数据去匹配，基本不可用</p><ul><li>SIMPLE 索引</li></ul><p>根据新数据所在的分区，获取受影响的分区文件列表，直接读取该部分 parquet 文件的 partition_key 和 record_key，与新数据执行 leftOutJoin 产生索引数据，性能低下（以 hoodieKey 为匹配键）</p><ul><li>HBASE 索引</li></ul><p>需要引入额外的 Hbase 服务，业务方基本不会允许</p><ul><li>GLOBAL_SIMPLE 索引</li></ul><p>与 SIMPLE 索引相比，GLOBAL_SIMPLE 索引读取的旧数据是全量数据，以 recordkey为匹配键，性能更低下 </p><h3 id="BLOOM"><a href="#BLOOM" class="headerlink" title="BLOOM"></a>BLOOM</h3><h4 id="写入信息"><a href="#写入信息" class="headerlink" title="写入信息"></a>写入信息</h4><p>org.apache.hudi.avro.HoodieBloomFilterWriteSupport#finalizeMetadata.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Map&lt;String, String&gt; <span class="title function_">finalizeMetadata</span><span class="params">()</span> &#123;</span><br><span class="line">    HashMap&lt;String, String&gt; extraMetadata = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    extraMetadata.put(HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY, bloomFilter.serializeToString());</span><br><span class="line">    <span class="keyword">if</span> (bloomFilter.getBloomFilterTypeCode().name().contains(HoodieDynamicBoundedBloomFilter.TYPE_CODE_PREFIX)) &#123;</span><br><span class="line">        extraMetadata.put(HOODIE_BLOOM_FILTER_TYPE_CODE, bloomFilter.getBloomFilterTypeCode().name());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (minRecordKey != <span class="literal">null</span> &amp;&amp; maxRecordKey != <span class="literal">null</span>) &#123;</span><br><span class="line">        extraMetadata.put(HOODIE_MIN_RECORD_KEY_FOOTER, minRecordKey.toString());</span><br><span class="line">        extraMetadata.put(HOODIE_MAX_RECORD_KEY_FOOTER, maxRecordKey.toString());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> extraMetadata;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这里会写入布隆过滤器序列化后的信息和主键的 MIN&#x2F;MAX 信息。<br>写入到 parquet 文件的 footer 中。</p><h4 id="读取信息"><a href="#读取信息" class="headerlink" title="读取信息"></a>读取信息</h4><p>org.apache.hudi.common.util.BaseFileUtils#readBloomFilterFromMetadata.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> BloomFilter <span class="title function_">readBloomFilterFromMetadata</span><span class="params">(Configuration configuration, Path filePath)</span> &#123;</span><br><span class="line">  Map&lt;String, String&gt; footerVals =</span><br><span class="line">      readFooter(configuration, <span class="literal">false</span>, filePath,</span><br><span class="line">          HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY,</span><br><span class="line">          HoodieAvroWriteSupport.OLD_HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY,</span><br><span class="line">          HoodieBloomFilterWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE);</span><br><span class="line">  <span class="type">String</span> <span class="variable">footerVal</span> <span class="operator">=</span> footerVals.get(HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY);</span><br><span class="line">  <span class="keyword">if</span> (<span class="literal">null</span> == footerVal) &#123;</span><br><span class="line">    <span class="comment">// We use old style key &quot;com.uber.hoodie.bloomfilter&quot;</span></span><br><span class="line">    footerVal = footerVals.get(HoodieAvroWriteSupport.OLD_HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">BloomFilter</span> <span class="variable">toReturn</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">if</span> (footerVal != <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (footerVals.containsKey(HoodieBloomFilterWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE)) &#123;</span><br><span class="line">      toReturn = BloomFilterFactory.fromString(footerVal,</span><br><span class="line">          footerVals.get(HoodieBloomFilterWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      toReturn = BloomFilterFactory.fromString(footerVal, BloomFilterTypeCode.SIMPLE.name());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> toReturn;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从 footer 中读取信息，构建布隆过滤器。</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>还是写入 footer 中，跟 Iceberg 差不多，只是 bloomfilter 的算法可能不同。</p><h3 id="GLOBAL-BLOOM"><a href="#GLOBAL-BLOOM" class="headerlink" title="GLOBAL_BLOOM"></a>GLOBAL_BLOOM</h3><p>全局索引强制跨表的所有分区的键的唯一性，即保证表中对于给定的记录键恰好存在一条记录。</p><h4 id="和-BLOOM-的区别"><a href="#和-BLOOM-的区别" class="headerlink" title="和 BLOOM 的区别"></a>和 BLOOM 的区别</h4><p>加载文件时加载全部分区的数据文件.<br>org.apache.hudi.index.bloom.HoodieGlobalBloomIndex#loadColumnRangesFromFiles</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">List&lt;Pair&lt;String, BloomIndexFileInfo&gt;&gt; <span class="title function_">loadColumnRangesFromFiles</span><span class="params">(List&lt;String&gt; partitions, <span class="keyword">final</span> HoodieEngineContext context,</span></span><br><span class="line"><span class="params">                                                                 <span class="keyword">final</span> HoodieTable hoodieTable)</span> &#123;</span><br><span class="line">  <span class="type">HoodieTableMetaClient</span> <span class="variable">metaClient</span> <span class="operator">=</span> hoodieTable.getMetaClient();</span><br><span class="line">  List&lt;String&gt; allPartitionPaths = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), metaClient.getBasePath());</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">super</span>.loadColumnRangesFromFiles(allPartitionPaths, context, hoodieTable);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>org.apache.hudi.index.bloom.HoodieBloomIndex#loadColumnRangesFromFiles.<br>只加载特定分区</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Pair&lt;String, BloomIndexFileInfo&gt;&gt; <span class="title function_">loadColumnRangesFromFiles</span><span class="params">(</span></span><br><span class="line"><span class="params">    List&lt;String&gt; partitions, <span class="keyword">final</span> HoodieEngineContext context, <span class="keyword">final</span> HoodieTable hoodieTable)</span> &#123;</span><br><span class="line">  <span class="comment">// Obtain the latest data files from all the partitions.</span></span><br><span class="line">  List&lt;Pair&lt;String, Pair&lt;String, HoodieBaseFile&gt;&gt;&gt; partitionPathFileIDList = getLatestBaseFilesForAllPartitions(partitions, context, hoodieTable).stream()</span><br><span class="line">      .map(pair -&gt; Pair.of(pair.getKey(), Pair.of(pair.getValue().getFileId(), pair.getValue())))</span><br><span class="line">      .collect(toList());</span><br><span class="line"></span><br><span class="line">  context.setJobStatus(<span class="built_in">this</span>.getClass().getName(), <span class="string">&quot;Obtain key ranges for file slices (range pruning=on): &quot;</span> + config.getTableName());</span><br><span class="line">  <span class="keyword">return</span> context.map(partitionPathFileIDList, pf -&gt; &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">HoodieRangeInfoHandle</span> <span class="variable">rangeInfoHandle</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HoodieRangeInfoHandle</span>(config, hoodieTable, Pair.of(pf.getKey(), pf.getValue().getKey()));</span><br><span class="line">      String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys(pf.getValue().getValue());</span><br><span class="line">      <span class="keyword">return</span> Pair.of(pf.getKey(), <span class="keyword">new</span> <span class="title class_">BloomIndexFileInfo</span>(pf.getValue().getKey(), minMaxKeys[<span class="number">0</span>], minMaxKeys[<span class="number">1</span>]));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (MetadataNotFoundException me) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;Unable to find range metadata in file :&quot;</span> + pf);</span><br><span class="line">      <span class="keyword">return</span> Pair.of(pf.getKey(), <span class="keyword">new</span> <span class="title class_">BloomIndexFileInfo</span>(pf.getValue().getKey()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, Math.max(partitionPathFileIDList.size(), <span class="number">1</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>这么看，GLOBAL_BLOOM 并不能实现比 BLOOM 更好的性能。</p><h3 id="BUCKET"><a href="#BUCKET" class="headerlink" title="BUCKET"></a>BUCKET</h3><p>推荐在较大数据量的场景下使用，避免布隆过滤器的假阳性问题。</p><h1 id="MetadataTable"><a href="#MetadataTable" class="headerlink" title="MetadataTable"></a>MetadataTable</h1><p>MetadataTable 是一张包含元数据的 mor 表，记录了数据表的文件、列统计、布隆过滤器的信息。</p><p>写入打标：</p><table border="1"><tr>  <td> engine </td>  <td> column_stat_idx </td>  <td> bloom_filter_idx </td>  <td> bucket_idx </td>  <td> flink_state </td>  <td> Simple </td>  <td> Hbase_idx </td></tr><tr>  <td> Spark </td>  <td> Y </td>  <td> Y </td>  <td> Y </td>  <td> N flink only </td>  <td> Y </td>  <td> Y </td></tr><tr>  <td> Spark </td>  <td> N </td>  <td> N </td>  <td> Y </td>  <td> Y </td>  <td> N spark only </td>  <td> N </td>  </tr></table><p>MetaDataTable表索引分区构建： </p><table border="1"><tr>  <td> engine </td>  <td> file_idx </td>  <td> column_stat_idx </td>  <td> bloom_filter_idx </td></tr><tr>  <td> Spark </td>  <td> Y </td>  <td> Y </td>  <td> Y </td></tr><tr>  <td> Spark </td>  <td> Y </td>  <td> Y </td>  <td> Y </td></tr></table><p>读取data skipping： </p><table border="1"><tr>  <td> engine </td>  <td> column_stat_idx </td>  <td> bloom_filter_idx </td>  <td> bucket_idx </td></tr><tr>  <td> Spark </td>  <td> Y </td>  <td> N </td>  <td> N </td></tr><tr>  <td> Spark </td>  <td> Y </td>  <td> N </td>  <td> N </td></tr></table><p>MetadataTable 的使用.<br>org.apache.hudi.HoodieFileIndex#lookupCandidateFilesInMetadataTable</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Computes pruned list of candidate base-files&#x27; names based on provided list of &#123;@link dataFilters&#125;</span></span><br><span class="line"><span class="comment"> * conditions, by leveraging Metadata Table&#x27;s Column Statistics index (hereon referred as ColStats for brevity)</span></span><br><span class="line"><span class="comment"> * bearing &quot;min&quot;, &quot;max&quot;, &quot;num_nulls&quot; statistics for all columns.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">NOTE:</span> This method has to return complete set of candidate files, since only provided candidates will</span></span><br><span class="line"><span class="comment"> * ultimately be scanned as part of query execution. Hence, this method has to maintain the</span></span><br><span class="line"><span class="comment"> * invariant of conservatively including every base-file&#x27;s name, that is NOT referenced in its index.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param queryFilters list of original data filters passed down from querying engine</span></span><br><span class="line"><span class="comment"> * @return list of pruned (data-skipped) candidate base-files&#x27; names</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">lookupCandidateFilesInMetadataTable</span></span>(queryFilters: <span class="type">Seq</span>[<span class="type">Expression</span>]): <span class="type">Try</span>[<span class="type">Option</span>[<span class="type">Set</span>[<span class="type">String</span>]]] = <span class="type">Try</span> &#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> Data Skipping is only effective when it references columns that are indexed w/in</span></span><br><span class="line">    <span class="comment">//       the Column Stats Index (CSI). Following cases could not be effectively handled by Data Skipping:</span></span><br><span class="line">    <span class="comment">//          - Expressions on top-level column&#x27;s fields (ie, for ex filters like &quot;struct.field &gt; 0&quot;, since</span></span><br><span class="line">    <span class="comment">//          CSI only contains stats for top-level columns, in this case for &quot;struct&quot;)</span></span><br><span class="line">    <span class="comment">//          - Any expression not directly referencing top-level column (for ex, sub-queries, since there&#x27;s</span></span><br><span class="line">    <span class="comment">//          nothing CSI in particular could be applied for)</span></span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> queryReferencedColumns = collectReferencedColumns(spark, queryFilters, schema)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!isMetadataTableEnabled || !isDataSkippingEnabled || !columnStatsIndex.isIndexAvailable) &#123;</span><br><span class="line">      validateConfig()</span><br><span class="line">      <span class="type">Option</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (queryFilters.isEmpty || queryReferencedColumns.isEmpty) &#123;</span><br><span class="line">      <span class="type">Option</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// <span class="doctag">NOTE:</span> Since executing on-cluster via Spark API has its own non-trivial amount of overhead,</span></span><br><span class="line">      <span class="comment">//       it&#x27;s most often preferential to fetch Column Stats Index w/in the same process (usually driver),</span></span><br><span class="line">      <span class="comment">//       w/o resorting to on-cluster execution.</span></span><br><span class="line">      <span class="comment">//       For that we use a simple-heuristic to determine whether we should read and process CSI in-memory or</span></span><br><span class="line">      <span class="comment">//       on-cluster: total number of rows of the expected projected portion of the index has to be below the</span></span><br><span class="line">      <span class="comment">//       threshold (of 100k records)</span></span><br><span class="line">      <span class="keyword">val</span> shouldReadInMemory = columnStatsIndex.shouldReadInMemory(<span class="keyword">this</span>, queryReferencedColumns)</span><br><span class="line"></span><br><span class="line">      columnStatsIndex.loadTransposed(queryReferencedColumns, shouldReadInMemory) &#123; transposedColStatsDF =&gt;</span><br><span class="line">        <span class="keyword">val</span> indexSchema = transposedColStatsDF.schema</span><br><span class="line">        <span class="keyword">val</span> indexFilter =</span><br><span class="line">        queryFilters.map(translateIntoColumnStatsIndexFilterExpr(_, indexSchema))</span><br><span class="line">        .reduce(<span class="type">And</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> allIndexedFileNames =</span><br><span class="line">        transposedColStatsDF.select(<span class="type">HoodieMetadataPayload</span>.<span class="type">COLUMN_STATS_FIELD_FILE_NAME</span>)</span><br><span class="line">        .collect()</span><br><span class="line">        .map(_.getString(<span class="number">0</span>))</span><br><span class="line">        .toSet</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> prunedCandidateFileNames =</span><br><span class="line">        transposedColStatsDF.where(<span class="keyword">new</span> <span class="type">Column</span>(indexFilter))</span><br><span class="line">        .select(<span class="type">HoodieMetadataPayload</span>.<span class="type">COLUMN_STATS_FIELD_FILE_NAME</span>)</span><br><span class="line">        .collect()</span><br><span class="line">        .map(_.getString(<span class="number">0</span>))</span><br><span class="line">        .toSet</span><br><span class="line"></span><br><span class="line">        <span class="comment">// <span class="doctag">NOTE:</span> Col-Stats Index isn&#x27;t guaranteed to have complete set of statistics for every</span></span><br><span class="line">        <span class="comment">//       base-file: since it&#x27;s bound to clustering, which could occur asynchronously</span></span><br><span class="line">        <span class="comment">//       at arbitrary point in time, and is not likely to be touching all of the base files.</span></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line">        <span class="comment">//       To close that gap, we manually compute the difference b/w all indexed (by col-stats-index)</span></span><br><span class="line">        <span class="comment">//       files and all outstanding base-files, and make sure that all base files not</span></span><br><span class="line">        <span class="comment">//       represented w/in the index are included in the output of this method</span></span><br><span class="line">        <span class="keyword">val</span> notIndexedFileNames = lookupFileNamesMissingFromIndex(allIndexedFileNames)</span><br><span class="line"></span><br><span class="line">        <span class="type">Some</span>(prunedCandidateFileNames ++ notIndexedFileNames)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><p>dataskip 的时候只使用了 column_stat_idx 信息。</p><h1 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h1><h3 id="TimelineService"><a href="#TimelineService" class="headerlink" title="TimelineService"></a>TimelineService</h3><p>构建 timeline 的时候避免从 HDFS 上读取数据文件。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;索引&quot;&gt;&lt;a href=&quot;#索引&quot; class=&quot;headerlink&quot; title=&quot;索引&quot;&gt;&lt;/a&gt;索引&lt;/h1&gt;&lt;p&gt;Hudi 支持多种索引：    &lt;/p&gt;
&lt;p&gt;HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE, </summary>
      
    
    
    
    
    <category term="数据湖" scheme="http://lvyanquan.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B9%96/"/>
    
  </entry>
  
  <entry>
    <title>一致性哈希</title>
    <link href="http://lvyanquan.github.io/2023/03/12/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/"/>
    <id>http://lvyanquan.github.io/2023/03/12/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/</id>
    <published>2023-03-12T02:22:41.000Z</published>
    <updated>2023-03-12T06:48:04.410Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>一致性哈希解决的是普通哈希水平拓展时需要全部迁移数据的问题。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ol><li>环形空间</li><li>映射服务器节点</li><li>映射数据</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;一致性哈希解决的是普通哈希水平拓展时需要全部迁移数据的问题。&lt;/p&gt;
&lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;he</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>两阶段提交</title>
    <link href="http://lvyanquan.github.io/2023/03/12/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/"/>
    <id>http://lvyanquan.github.io/2023/03/12/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/</id>
    <published>2023-03-12T02:22:41.000Z</published>
    <updated>2023-04-26T09:16:57.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h1><p>分布式事务是指跨越多个计算机或者多个网络节点的事务。在分布式系统中，一个事务可能需要访问和修改多个节点上的数据，而且这些节点可能分布在不同的物理位置上。由于存在多个节点，因此需要确保事务的原子性、一致性、隔离性和持久性，这些性质被称为 ACID 属性。</p><p>分布式事务需要通过协调器（coordinator）来实现。协调器负责协调所有参与者节点的操作，确保所有节点都遵循相同的事务处理流程，并最终提交或回滚事务。协调器需要处理节点故障、网络分区等复杂情况，保证分布式事务的正确性和可靠性。</p><p>分布式事务的实现方式包括两阶段提交协议（Two-Phase Commit，2PC）、三阶段提交协议（Three-Phase Commit，3PC）、Paxos 协议等。这些协议各有优缺点，应根据具体情况进行选择。</p><h1 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h1><p>两阶段提交（Two-Phase Commit，2PC）是一种用于实现分布式事务的协议。它由一个协调者（coordinator）和多个参与者（participant）组成。</p><p>在 2PC 协议中，事务分为两个阶段：准备阶段（Prepare Phase）和提交阶段（Commit Phase）。</p><p>准备阶段<br>在准备阶段，协调者向所有参与者发送请求，要求参与者准备好执行该事务，并将准备结果通知给协调者。参与者在接收到请求后，会执行本地的事务操作，并将执行结果返回给协调者。在这个阶段，如果有任何一个参与者无法正常响应，则协调者会向所有参与者发送回滚请求，取消该事务的执行。</p><p>提交阶段<br>如果所有参与者都已经准备好执行该事务，并且协调者已经确认了所有参与者的准备结果，那么事务就进入提交阶段。在这个阶段，协调者向所有参与者发送提交请求，要求参与者执行该事务，并将执行结果通知给协调者。如果所有参与者都成功地执行了该事务，那么协调者就向所有参与者发送提交完成的消息，表示该事务已经成功提交。如果在提交阶段中任何一个参与者无法执行该事务，那么协调者就向所有参与者发送回滚请求，取消该事务的执行。</p><p>2PC 协议的优点是简单、可靠。它可以确保所有参与者要么全部执行该事务，要么全部取消该事务，从而保证了分布式事务的 ACID 特性。然而，2PC 协议也存在一些缺点，例如：</p><p>参与者在等待协调者的通知时，可能会一直处于阻塞状态，影响系统的吞吐量。</p><p>协调者在发生故障时，可能会导致整个系统无法正常工作。</p><p>2PC 协议的执行效率较低，因为需要进行两个阶段的通信和协调。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;分布式事务&quot;&gt;&lt;a href=&quot;#分布式事务&quot; class=&quot;headerlink&quot; title=&quot;分布式事务&quot;&gt;&lt;/a&gt;分布式事务&lt;/h1&gt;&lt;p&gt;分布式事务是指跨越多个计算机或者多个网络节点的事务。在分布式系统中，一个事务可能需要访问和修改多个节点上的数据，而且</summary>
      
    
    
    
    
    <category term="分布式" scheme="http://lvyanquan.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="大数据" scheme="http://lvyanquan.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>面试题 17.05. 字母与数字</title>
    <link href="http://lvyanquan.github.io/2023/03/12/%E9%9D%A2%E8%AF%95%E9%A2%98%2017.05.%20%E5%AD%97%E6%AF%8D%E4%B8%8E%E6%95%B0%E5%AD%97/"/>
    <id>http://lvyanquan.github.io/2023/03/12/%E9%9D%A2%E8%AF%95%E9%A2%98%2017.05.%20%E5%AD%97%E6%AF%8D%E4%B8%8E%E6%95%B0%E5%AD%97/</id>
    <published>2023-03-12T02:22:41.000Z</published>
    <updated>2023-04-26T09:17:26.195Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://leetcode.cn/problems/find-longest-subarray-lcci/description/">面试题 17.05. 字母与数字</a></p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定一个放有字母和数字的数组，找到最长的子数组，且包含的字母和数字的个数相同。</p><p>返回该子数组，若存在多个最长子数组，返回左端点下标值最小的子数组。若不存在这样的数组，返回一个空数组。</p><p>示例 1:</p><p>输入: [“A”,”1”,”B”,”C”,”D”,”2”,”3”,”4”,”E”,”5”,”F”,”G”,”6”,”7”,”H”,”I”,”J”,”K”,”L”,”M”]</p><p>输出: [“A”,”1”,”B”,”C”,”D”,”2”,”3”,”4”,”E”,”5”,”F”,”G”,”6”,”7”]    </p><p>示例 2:</p><p>输入: [“A”,”A”]</p><p>输出: []</p><p>提示：</p><p>  array.length &lt;&#x3D; 100000</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol><li>看到 100000 这个数量就知道不能用 n2 的复杂度了，而且这题没有明显的二分特点，所以只能考虑 n 的时间复杂度，那么只能考虑使用哈希表降低搜索范围了。    </li><li>但是哈希表里应该保存什么呢？因为我们要求的是子数组里数字和字符数量相同，那么在枚举右端点的时候，我们需要快速找到能够让区间满足要求的左端点。这引导我们考虑采用前缀和的方式做处理，只要右端点的“数字出现的次数 - 字符串出现的次数”的差值等于左端点就可以满足要求了。<br>前缀和的计算方式：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key += array[i].charAt(<span class="number">0</span>) &gt;= <span class="string">&#x27;0&#x27;</span> &amp;&amp; array[i].charAt(<span class="number">0</span>) &lt;= <span class="string">&#x27;9&#x27;</span> ? <span class="number">1</span> : -<span class="number">1</span>;</span><br></pre></td></tr></table></figure></li><li>因为题目要求在区间长度相同时返回最左区间，所以哈希表的记录的是键为“数字出现的次数 - 字符串出现的次数”，值为这个键出现最早的位置。</li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String[] findLongestSubarray(String[] array) &#123;</span><br><span class="line">        Map&lt;Integer, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="type">int</span> <span class="variable">key</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>, len = <span class="number">0</span>;</span><br><span class="line">        map.put(<span class="number">0</span>, -<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;array.length;i++) &#123;</span><br><span class="line">            key += array[i].charAt(<span class="number">0</span>) &gt;= <span class="string">&#x27;0&#x27;</span> &amp;&amp; array[i].charAt(<span class="number">0</span>) &lt;= <span class="string">&#x27;9&#x27;</span> ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(map.containsKey(key)) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">pos</span> <span class="operator">=</span> map.get(key);</span><br><span class="line">                <span class="keyword">if</span>( (len &lt; i - pos) || ((len == i - pos) &amp;&amp; pos &lt; left) )&#123;</span><br><span class="line">                    len = i - pos;</span><br><span class="line">                    left = pos + <span class="number">1</span>;</span><br><span class="line">                &#125; </span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                map.put(key, i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Arrays.copyOfRange(array, left, left + len);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://leetcode.cn/problems/find-longest-subarray-lcci/description/&quot;&gt;面试题 17.05. 字母与数字&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;</summary>
      
    
    
    
    
    <category term="算法" scheme="http://lvyanquan.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
